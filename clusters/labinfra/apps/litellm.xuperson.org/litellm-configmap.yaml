apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config-file
  namespace: litellm
data:
  config.yaml: |
    # Advanced routing: specific models, wildcards, and default rotation with fallbacks
    model_list: 
      
      # === SPECIFIC MODEL ROUTING ===
      # Direct routing to specific models when requested by exact name
      
      # Gemini 2.5 Pro - Enhanced thinking and reasoning
      - model_name: "gemini-2.5-pro"
        litellm_params:
          model: "gemini/gemini-2.5-pro"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 5
          tpm: 250000
      - model_name: "gemini-2.5-pro"
        litellm_params:
          model: "gemini/gemini-2.5-pro"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 5
          tpm: 250000
          
      # Gemini 2.5 Flash - Adaptive thinking, cost efficient
      - model_name: "gemini-2.5-flash"
        litellm_params:
          model: "gemini/gemini-2.5-flash"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 10
          tpm: 250000
      - model_name: "gemini-2.5-flash"
        litellm_params:
          model: "gemini/gemini-2.5-flash"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 10
          tpm: 250000
          
      # Gemini 2.5 Flash-Lite - Most cost-efficient
      - model_name: "gemini-2.5-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.5-flash-lite"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 250000
      - model_name: "gemini-2.5-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.5-flash-lite"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 250000
          
      # Gemini 2.0 Flash - High throughput
      - model_name: "gemini-2.0-flash"
        litellm_params:
          model: "gemini/gemini-2.0-flash"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 1000000
      - model_name: "gemini-2.0-flash"
        litellm_params:
          model: "gemini/gemini-2.0-flash"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 1000000
          
      # Gemini 2.0 Flash Experimental - Higher rate limits (correct model ID)
      - model_name: "gemini-2.0-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.0-flash-exp"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 1000000
      - model_name: "gemini-2.0-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.0-flash-exp"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 1000000
          
      # Gemma 3n E2B - Efficient 2B model
      - model_name: "gemma-3n-e2b-it"
        litellm_params:
          model: "gemini/gemma-3n-e2b-it"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gemma-3n-e2b-it"
        litellm_params:
          model: "gemini/gemma-3n-e2b-it"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
          
      # Gemma 3n E4B - Efficient 4B model  
      - model_name: "gemma-3n-e4b-it"
        litellm_params:
          model: "gemini/gemma-3n-e4b-it"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gemma-3n-e4b-it"
        litellm_params:
          model: "gemini/gemma-3n-e4b-it"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
          
      # Gemma 3 27B - Large model for complex tasks
      - model_name: "gemma-3-27b-it"
        litellm_params:
          model: "gemini/gemma-3-27b-it"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gemma-3-27b-it"
        litellm_params:
          model: "gemini/gemma-3-27b-it"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
          
      # === WILDCARD ROUTING ===
      # Handles any gemini/* model requests (e.g., gemini/gemini-2.5-pro, gemini/new-model)
      - model_name: "gemini/*"
        litellm_params:
          model: "gemini/*"
          api_key: "os.environ/GEMINI_KEY_1"
      - model_name: "gemini/*"
        litellm_params:
          model: "gemini/*"
          api_key: "os.environ/GEMINI_KEY_2"
          
      # === DEFAULT ROTATION POOL ===
      # "gpt-4" and catch-all models rotate through all available models
      
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-pro"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 5
          tpm: 250000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-pro"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 5
          tpm: 250000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-flash"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 10
          tpm: 250000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-flash"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 10
          tpm: 250000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-flash-lite"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 250000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-flash-lite"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 250000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.0-flash"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 1000000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.0-flash"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 1000000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.0-flash-exp"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 1000000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.0-flash-exp"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 1000000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3n-e2b-it"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3n-e2b-it"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3n-e4b-it"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3n-e4b-it"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3-27b-it"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3-27b-it"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
    
    # Router settings for quota rotation and rate limit handling
    router_settings:
      routing_strategy: simple-shuffle          # Rotate through all deployments
      num_retries: 8                           # More retries for rate limits/quota
      timeout: 60                              # Timeout per request
      retry_after: 0.5                         # Faster retry for quota rotation
      cooldown_time: 5                         # Shorter cooldown for faster recovery
      allowed_fails: 5                         # More lenient for rate limits
      
      # Enable fallbacks on quota/rate limit errors
      enable_pre_call_checks: true             # Check deployment health before routing
      disable_spend_logs: false                # Keep spend tracking
      
      # Specific retry policies for different error types
      retry_policy:
        "RateLimitErrorRetries": 8             # More retries for rate limits
        "ResourceExhaustedErrorRetries": 8     # Handle quota exhaustion
        "ServiceUnavailableErrorRetries": 8    # Handle service unavailable
        "BadRequestErrorRetries": 2            # Fewer retries for bad requests
        "AuthenticationErrorRetries": 1        # Don't retry auth errors
        
      # Specific cooldown policies for different error types  
      allowed_fails_policy:
        "RateLimitErrorAllowedFails": 100      # Very lenient for rate limits
        "ResourceExhaustedErrorAllowedFails": 100  # Very lenient for quota exhaustion
        "ServiceUnavailableErrorAllowedFails": 100 # Very lenient for service issues
        "BadRequestErrorAllowedFails": 2       # Cooldown quickly for bad requests
        "AuthenticationErrorAllowedFails": 1   # Cooldown immediately for auth errors
    
    # Advanced fallback configuration
    litellm_settings:
      # Specific model fallbacks - fall back to gpt-4 rotation pool
      fallbacks: 
        - "gemini-2.5-pro": ["gpt-4"]
        - "gemini-2.5-flash": ["gpt-4"] 
        - "gemini-2.5-flash-lite": ["gpt-4"]
        - "gemini-2.0-flash": ["gpt-4"]
        - "gemini-2.0-flash-lite": ["gpt-4"]
        - "gemma-3n-e2b-it": ["gpt-4"]
        - "gemma-3n-e4b-it": ["gpt-4"]
        - "gemma-3-27b-it": ["gpt-4"]
        - "gemini/*": ["gpt-4"]                  # Wildcard fallback to rotation
      
      # Default fallback for any unrecognized model
      default_fallbacks: ["gpt-4"]
      
      # Fallback error conditions - trigger fallbacks on quota/rate limit errors
      context_window_fallbacks: [
        {"RateLimitError": ["gpt-4"]},
        {"ResourceExhaustedError": ["gpt-4"]}, 
        {"ServiceUnavailableError": ["gpt-4"]},
        {"QuotaExceededError": ["gpt-4"]}
      ]
      
      # General settings
      success_callback: ["langfuse"]           # Optional: track successful requests
      failure_callback: ["langfuse"]           # Optional: track failed requests
      drop_params: true                        # Drop unsupported parameters
      set_verbose: false                       # Reduce logging noise
      
      # Enable detailed error handling
      request_timeout: 60                      # Request timeout
      max_retries: 8                           # Max retries before fallback
      fallback_immediately_on_429: true       # Immediate fallback on rate limits
      fallback_immediately_on_503: true       # Immediate fallback on service unavailable