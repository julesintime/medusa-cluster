apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config-file
  namespace: litellm
data:
  config.yaml: |
    # Advanced routing: specific models, wildcards, and default rotation with fallbacks
    model_list: 
      
      # === SPECIFIC MODEL ROUTING ===
      # Direct routing to specific models when requested by exact name
      
      # Gemini 2.5 Pro - Enhanced thinking and reasoning
      - model_name: "gemini-2.5-pro"
        litellm_params:
          model: "gemini/gemini-2.5-pro"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 5
          tpm: 250000
      - model_name: "gemini-2.5-pro"
        litellm_params:
          model: "gemini/gemini-2.5-pro"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 5
          tpm: 250000
          
      # Gemini 2.5 Flash - Adaptive thinking, cost efficient
      - model_name: "gemini-2.5-flash"
        litellm_params:
          model: "gemini/gemini-2.5-flash"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 10
          tpm: 250000
      - model_name: "gemini-2.5-flash"
        litellm_params:
          model: "gemini/gemini-2.5-flash"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 10
          tpm: 250000
          
      # Gemini 2.5 Flash-Lite - Most cost-efficient
      - model_name: "gemini-2.5-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.5-flash-lite"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 250000
      - model_name: "gemini-2.5-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.5-flash-lite"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 250000
          
      # Gemini 2.0 Flash - High throughput
      - model_name: "gemini-2.0-flash"
        litellm_params:
          model: "gemini/gemini-2.0-flash"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 1000000
      - model_name: "gemini-2.0-flash"
        litellm_params:
          model: "gemini/gemini-2.0-flash"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 1000000
          
      # Gemini 2.0 Flash-Lite - Higher rate limits  
      - model_name: "gemini-2.0-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.0-flash-lite"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 1000000
      - model_name: "gemini-2.0-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.0-flash-lite"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 1000000
          
      # Gemma 3 - Open model
      - model_name: "gemma-3"
        litellm_params:
          model: "gemini/gemma-3"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gemma-3"
        litellm_params:
          model: "gemini/gemma-3"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
          
      # Gemma 3n - Open model variant
      - model_name: "gemma-3n"
        litellm_params:
          model: "gemini/gemma-3n"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gemma-3n"
        litellm_params:
          model: "gemini/gemma-3n"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
          
      # === WILDCARD ROUTING ===
      # Handles any gemini/* model requests (e.g., gemini/gemini-2.5-pro, gemini/new-model)
      - model_name: "gemini/*"
        litellm_params:
          model: "gemini/*"
          api_key: "os.environ/GEMINI_KEY_1"
      - model_name: "gemini/*"
        litellm_params:
          model: "gemini/*"
          api_key: "os.environ/GEMINI_KEY_2"
          
      # === DEFAULT ROTATION POOL ===
      # "gpt-4" and catch-all models rotate through all available models
      
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-pro"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 5
          tpm: 250000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-pro"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 5
          tpm: 250000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-flash"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 10
          tpm: 250000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-flash"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 10
          tpm: 250000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-flash-lite"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 250000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.5-flash-lite"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 250000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.0-flash"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 15
          tpm: 1000000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.0-flash"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 15
          tpm: 1000000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.0-flash-lite"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 1000000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemini-2.0-flash-lite"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 1000000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
          
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3n"
          api_key: "os.environ/GEMINI_KEY_1"
          rpm: 30
          tpm: 15000
      - model_name: "gpt-4"
        litellm_params:
          model: "gemini/gemma-3n"
          api_key: "os.environ/GEMINI_KEY_2"
          rpm: 30
          tpm: 15000
    
    # Router settings for quota rotation and rate limit handling
    router_settings:
      routing_strategy: simple-shuffle          # Rotate through all deployments
      num_retries: 5                           # Try multiple deployments for rate limits
      timeout: 60                              # Timeout per request
      retry_after: 1                           # Min 1s delay before retrying (helps with rate limits)
      cooldown_time: 10                        # Short cooldown for quota rotation (10s)
      allowed_fails: 3                         # Allow more fails before cooldown (rate limits are common)
      
      # Specific retry policies for different error types
      retry_policy:
        "RateLimitErrorRetries": 5             # Retry rate limit errors across deployments
        "BadRequestErrorRetries": 2            # Fewer retries for bad requests
        "AuthenticationErrorRetries": 1        # Don't retry auth errors
        
      # Specific cooldown policies for different error types  
      allowed_fails_policy:
        "RateLimitErrorAllowedFails": 50       # Allow many rate limit errors before cooldown
        "BadRequestErrorAllowedFails": 2       # Cooldown quickly for bad requests
        "AuthenticationErrorAllowedFails": 1   # Cooldown immediately for auth errors
    
    # Advanced fallback configuration
    litellm_settings:
      # Specific model fallbacks - fall back to gpt-4 rotation pool
      fallbacks: 
        - "gemini-2.5-pro": ["gpt-4"]
        - "gemini-2.5-flash": ["gpt-4"] 
        - "gemini-2.5-flash-lite": ["gpt-4"]
        - "gemini-2.0-flash": ["gpt-4"]
        - "gemini-2.0-flash-lite": ["gpt-4"]
        - "gemma-3": ["gpt-4"]
        - "gemma-3n": ["gpt-4"]
        - "gemini/*": ["gpt-4"]                  # Wildcard fallback to rotation
      
      # Default fallback for any unrecognized model
      default_fallbacks: ["gpt-4"]
      
      # General settings
      success_callback: ["langfuse"]           # Optional: track successful requests
      failure_callback: ["langfuse"]           # Optional: track failed requests
      drop_params: true                        # Drop unsupported parameters
      set_verbose: false                       # Reduce logging noise