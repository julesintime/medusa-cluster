---
# Source: trigger/charts/clickhouse/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: trigger-clickhouse
  namespace: "trigger"
  labels:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/version: 25.6.1
    helm.sh/chart: clickhouse-9.3.7
    app.kubernetes.io/part-of: clickhouse
    app.kubernetes.io/component: clickhouse
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: trigger
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/part-of: clickhouse
      app.kubernetes.io/component: clickhouse
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 8123
        - port: 9000
        - port: 9004
        - port: 9005
        - port: 9009
---
# Source: trigger/charts/clickhouse/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: trigger-clickhouse-shard0
  namespace: "trigger"
  labels:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/version: 25.6.1
    helm.sh/chart: clickhouse-9.3.7
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/part-of: clickhouse
    shard: "0"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: trigger
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/component: clickhouse
      app.kubernetes.io/part-of: clickhouse
      shard: "0"
---
# Source: trigger/charts/clickhouse/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: trigger-clickhouse
  namespace: "trigger"
  labels:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/version: 25.6.1
    helm.sh/chart: clickhouse-9.3.7
    app.kubernetes.io/component: clickhouse
automountServiceAccountToken: false
---
# Source: trigger/templates/supervisor.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: trigger-supervisor
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: supervisor
---
# Source: trigger/templates/webapp.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: trigger-webapp
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: webapp
---
# Source: trigger/charts/clickhouse/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: trigger-clickhouse
  namespace: "trigger"
  labels:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/version: 25.6.1
    helm.sh/chart: clickhouse-9.3.7
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/part-of: clickhouse
type: Opaque
data:
  admin-password: "cGFzc3dvcmQ="
---
# Source: trigger/charts/clickhouse/templates/configd-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: trigger-clickhouse-configd
  namespace: "trigger"
  labels:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/version: 25.6.1
    helm.sh/chart: clickhouse-9.3.7
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/part-of: clickhouse
data:
  override.xml: |
    <clickhouse>
      <logger>
        <level>warning</level>
      </logger>
      <!-- Official recommendations for systems with <16GB RAM -->
      <mark_cache_size>524288000</mark_cache_size> <!-- 500MB -->
      <concurrent_threads_soft_limit_num>1</concurrent_threads_soft_limit_num>
      <profiles>
        <default>
          <max_block_size>8192</max_block_size>
          <max_download_threads>1</max_download_threads>
          <input_format_parallel_parsing>0</input_format_parallel_parsing>
          <output_format_parallel_formatting>0</output_format_parallel_formatting>
        </default>
      </profiles>
    </clickhouse>
    
  01-listen.xml: |
    <yandex>
        <listen_host>::</listen_host>
        <listen_host>0.0.0.0</listen_host>
        <listen_try>1</listen_try>
    </yandex>
  02-logger.xml: |
    <yandex>
        <logger>
            <level>information</level>
        </logger>
    </yandex>
  03-macros.xml: |
    <yandex>
        <macros>
            <shard from_env="CLICKHOUSE_SHARD_ID"></shard>
            <replica from_env="CLICKHOUSE_REPLICA_ID"></replica>
            <layer>trigger-clickhouse</layer>
        </macros>
    </yandex>
  08-sampling.xml: |
    <yandex>
        <asynchronous_insert_log remove="1"/>
        <asynchronous_metric_log remove="1"/>
        <error_log remove="1"/>
        <event_log remove="1"/>
        <latency_log remove="1"/>
        <metric_log remove="1"/>
        <query_log remove="1"/>
        <query_metric_log remove="1"/>
        <query_thread_log remove="1"/>
        <query_views_log remove="1"/>
        <part_log remove="1"/>
        <text_log remove="1"/>
        <trace_log remove="1"/>
        <opentelemetry_span_log remove="1"/>
        <processors_profile_log remove="1"/>
    </yandex>
---
# Source: trigger/templates/webapp.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: trigger-shared
  annotations:
    "helm.sh/resource-policy": keep
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Mi
---
# Source: trigger/templates/supervisor.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: trigger-supervisor-trigger
  namespace: trigger
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: supervisor
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["create", "delete", "deletecollection", "get", "list", "watch"]
---
# Source: trigger/templates/webapp.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: trigger-webapp-token-syncer
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: webapp
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["create", "get", "update", "patch"]
---
# Source: trigger/templates/supervisor.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: trigger-supervisor-trigger-binding
  namespace: trigger
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: supervisor
subjects:
  - kind: ServiceAccount
    name: trigger-supervisor
    namespace: trigger
roleRef:
  kind: Role
  name: trigger-supervisor-trigger
  apiGroup: rbac.authorization.k8s.io
---
# Source: trigger/templates/webapp.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: trigger-webapp-token-syncer
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: webapp
subjects:
  - kind: ServiceAccount
    name: trigger-webapp
    namespace: trigger
roleRef:
  kind: Role
  name: trigger-webapp-token-syncer
  apiGroup: rbac.authorization.k8s.io
---
# Source: trigger/charts/clickhouse/templates/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: trigger-clickhouse-headless
  namespace: "trigger"
  labels:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/version: 25.6.1
    helm.sh/chart: clickhouse-9.3.7
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/part-of: clickhouse
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: http
      targetPort: http
      port: 8123
      protocol: TCP
    - name: tcp
      targetPort: tcp
      port: 9000
      protocol: TCP
    - name: tcp-mysql
      targetPort: tcp-mysql
      port: 9004
      protocol: TCP
    - name: tcp-postgresql
      targetPort: tcp-postgresql
      port: 9005
      protocol: TCP
    - name: http-intersrv
      targetPort: http-intersrv
      port: 9009
      protocol: TCP
  selector:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/part-of: clickhouse
---
# Source: trigger/charts/clickhouse/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: trigger-clickhouse
  namespace: "trigger"
  labels:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/version: 25.6.1
    helm.sh/chart: clickhouse-9.3.7
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/part-of: clickhouse
  annotations:
    {}
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: http
      targetPort: http
      port: 8123
      protocol: TCP
      nodePort: null
    - name: tcp
      targetPort: tcp
      port: 9000
      protocol: TCP
      nodePort: null
    - name: tcp-mysql
      targetPort: tcp-mysql
      port: 9004
      protocol: TCP
      nodePort: null
    - name: tcp-postgresql
      targetPort: tcp-postgresql
      port: 9005
      protocol: TCP
      nodePort: null
    - name: http-intersrv
      targetPort: http-intersrv
      port: 9009
      protocol: TCP
      nodePort: null
  selector:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/part-of: clickhouse
---
# Source: trigger/templates/electric.yaml
apiVersion: v1
kind: Service
metadata:
  name: trigger-electric
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: electric
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/component: electric
---
# Source: trigger/templates/supervisor.yaml
apiVersion: v1
kind: Service
metadata:
  name: trigger-supervisor
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: supervisor
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: workload
      protocol: TCP
      name: workload
    - port: 9088
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/component: supervisor
---
# Source: trigger/templates/webapp.yaml
apiVersion: v1
kind: Service
metadata:
  name: trigger-webapp
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: webapp
spec:
  type: ClusterIP
  ports:
    - port: 3030
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/component: webapp
---
# Source: trigger/templates/electric.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trigger-electric
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: electric
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: trigger
      app.kubernetes.io/instance: trigger
      app.kubernetes.io/component: electric
  template:
    metadata:
      labels:
        app.kubernetes.io/name: trigger
        app.kubernetes.io/instance: trigger
        app.kubernetes.io/component: electric
    spec:
      containers:
        - name: electric
          image: "docker.io/electricsql/electric:1.0.24"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: trigger-secrets
                  key: postgres-database-url
            - name: ELECTRIC_INSECURE
              value: "true"
            - name: ELECTRIC_USAGE_REPORTING
              value: "false"
          livenessProbe:
            httpGet:
              path: /v1/health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: /v1/health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
            successThreshold: 1
          resources:
            {}
---
# Source: trigger/templates/supervisor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trigger-supervisor
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: supervisor
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: trigger
      app.kubernetes.io/instance: trigger
      app.kubernetes.io/component: supervisor
  template:
    metadata:
      labels:
        app.kubernetes.io/name: trigger
        app.kubernetes.io/instance: trigger
        app.kubernetes.io/component: supervisor
    spec:
      serviceAccountName: trigger-supervisor
      securityContext:
        fsGroup: 1000
      containers:
        - name: supervisor
          image: ghcr.io/triggerdotdev/supervisor:v4.0.0
          imagePullPolicy: IfNotPresent
          ports:
            - name: workload
              containerPort: 3000
              protocol: TCP
            - name: metrics
              containerPort: 9088
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: workload
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: /health
              port: workload
            initialDelaySeconds: 15
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          resources:
            {}
          env:
            # Core configuration
            - name: TRIGGER_API_URL
              value: "http://trigger-webapp:3030"
            - name: TRIGGER_WORKER_TOKEN
              valueFrom:
                secretKeyRef:
                  name: trigger-worker-token
                  key: token
            # Worker instance configuration
            - name: TRIGGER_WORKER_INSTANCE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # Kubernetes configuration
            - name: KUBERNETES_NAMESPACE
              value: "trigger"
            - name: KUBERNETES_FORCE_ENABLED
              value: "true"
            - name: KUBERNETES_WORKER_NODETYPE_LABEL
              value: ""
            - name: KUBERNETES_EPHEMERAL_STORAGE_SIZE_LIMIT
              value: "10Gi"
            - name: KUBERNETES_EPHEMERAL_STORAGE_SIZE_REQUEST
              value: "2Gi"
            # Pod cleaner configuration
            - name: POD_CLEANER_ENABLED
              value: "true"
            - name: POD_CLEANER_BATCH_SIZE
              value: "100"
            - name: POD_CLEANER_INTERVAL_MS
              value: "10000"
            # Failed pod handler
            - name: FAILED_POD_HANDLER_ENABLED
              value: "true"
            - name: FAILED_POD_HANDLER_RECONNECT_INTERVAL_MS
              value: "1000"
            # Workload API configuration
            - name: TRIGGER_WORKLOAD_API_PROTOCOL
              value: "http"
            - name: TRIGGER_WORKLOAD_API_DOMAIN
              value: "trigger-supervisor.trigger.svc.cluster.local"
            - name: TRIGGER_WORKLOAD_API_PORT_EXTERNAL
              value: "3000"
            - name: TRIGGER_WORKLOAD_API_PORT_INTERNAL
              value: "3000"
            - name: TRIGGER_WORKLOAD_API_HOST_INTERNAL
              value: "0.0.0.0"
            - name: TRIGGER_WORKLOAD_API_ENABLED
              value: "true"
            # Dequeue configuration
            - name: TRIGGER_DEQUEUE_ENABLED
              value: "true"
            - name: TRIGGER_DEQUEUE_INTERVAL_MS
              value: "250"
            - name: TRIGGER_DEQUEUE_MAX_RUN_COUNT
              value: "100"
            - name: TRIGGER_DEQUEUE_IDLE_INTERVAL_MS
              value: "500"
            # Heartbeat configuration
            - name: RUNNER_HEARTBEAT_INTERVAL_SECONDS
              value: "30"
            - name: RUNNER_SNAPSHOT_POLL_INTERVAL_SECONDS
              value: "30"
            # Metrics configuration
            - name: METRICS_ENABLED
              value: "true"
            - name: METRICS_COLLECT_DEFAULTS
              value: "true"
            - name: METRICS_HOST
              value: "0.0.0.0"
            - name: METRICS_PORT
              value: "9088"
            # Debug
            - name: DEBUG
              value: "false"
            # OTEL
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "http://trigger-webapp:3030/otel"
---
# Source: trigger/templates/webapp.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trigger-webapp
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: webapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: trigger
      app.kubernetes.io/instance: trigger
      app.kubernetes.io/component: webapp
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: webapp
      labels:
        app.kubernetes.io/name: trigger
        app.kubernetes.io/instance: trigger
        app.kubernetes.io/component: webapp
    spec:
      serviceAccountName: trigger-webapp
      securityContext:
        fsGroup: 1000
      initContainers:
        - name: init-shared
          image: busybox:1.35
          command: ['sh', '-c', 'mkdir -p /home/node/shared']
          securityContext:
            runAsUser: 1000
          volumeMounts:
            - name: shared
              mountPath: /home/node/shared
      containers:
        - name: token-syncer
          image: bitnami/kubectl:1.28
          securityContext:
            runAsUser: 1000
            runAsNonRoot: true
          command:
            - /bin/bash
            - -c
            - |
              TOKEN_FILE="/home/node/shared/worker_token"
              SECRET_NAME="trigger-worker-token"
              NAMESPACE="trigger"
              
              echo "Token syncer starting..."
              echo "Monitoring: $TOKEN_FILE"
              echo "Target secret: $SECRET_NAME"
              
              while true; do
                if [ -f "$TOKEN_FILE" ]; then
                  TOKEN=$(cat "$TOKEN_FILE")
                  if [ ! -z "$TOKEN" ]; then
                    echo "Token file found, creating/updating secret..."
                    
                    # Create or update the secret
                    kubectl create secret generic "$SECRET_NAME" \
                      --from-literal=token="$TOKEN" \
                      --namespace="$NAMESPACE" \
                      --dry-run=client -o yaml | kubectl apply -f -
                    
                    if [ $? -eq 0 ]; then
                      echo "Secret successfully created/updated"
                      # Continue monitoring for updates
                      sleep 30
                    else
                      echo "Failed to create/update secret, retrying in 5s..."
                      sleep 5
                    fi
                  else
                    echo "Token file exists but is empty, waiting..."
                    sleep 2
                  fi
                else
                  echo "Waiting for token file..."
                  sleep 2
                fi
              done
          volumeMounts:
            - name: shared
              mountPath: /home/node/shared
        - name: webapp
          securityContext:
            null
          image: ghcr.io/triggerdotdev/trigger.dev:v4.0.0
          imagePullPolicy: IfNotPresent
          command: 
            - ./scripts/entrypoint.sh
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: http
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5
            successThreshold: 1
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: http
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5
            successThreshold: 1
          resources:
            {}
          env:
            - name: APP_ORIGIN
              value: "https://trigger.xuperson.org"
            - name: LOGIN_ORIGIN
              value: "https://trigger.xuperson.org"
            - name: API_ORIGIN
              value: "http://localhost:3040"
            - name: ELECTRIC_ORIGIN
              value: "http://trigger-electric:3000"
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: trigger-secrets
                  key: postgres-database-url
            - name: DIRECT_URL
              valueFrom:
                secretKeyRef:
                  name: trigger-secrets
                  key: postgres-direct-url
            - name: REDIS_HOST
              value: "trigger-redis-master"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_TLS_DISABLED
              value: "true"
            - name: APP_LOG_LEVEL
              value: "info"
            - name: DEV_OTEL_EXPORTER_OTLP_ENDPOINT
              value: "https://trigger.xuperson.org/otel"
            - name: DEPLOY_REGISTRY_HOST
              value: "localhost:5001"
            - name: DEPLOY_REGISTRY_NAMESPACE
              value: "trigger"
            - name: OBJECT_STORE_BASE_URL
              value: "http://trigger-minio:9000"
            - name: GRACEFUL_SHUTDOWN_TIMEOUT
              value: "1000"
            - name: TRIGGER_BOOTSTRAP_ENABLED
              value: "1"
            - name: TRIGGER_BOOTSTRAP_WORKER_GROUP_NAME
              value: "bootstrap"
            - name: TRIGGER_BOOTSTRAP_WORKER_TOKEN_PATH
              value: "/home/node/shared/worker_token"
            - name: TASK_PAYLOAD_OFFLOAD_THRESHOLD
              value: "524288"
            - name: TASK_PAYLOAD_MAXIMUM_SIZE
              value: "3.145728e+06"
            - name: BATCH_TASK_PAYLOAD_MAXIMUM_SIZE
              value: "1e+06"
            - name: TASK_RUN_METADATA_MAXIMUM_SIZE
              value: "262144"
            - name: DEFAULT_ENV_EXECUTION_CONCURRENCY_LIMIT
              value: "100"
            - name: DEFAULT_ORG_EXECUTION_CONCURRENCY_LIMIT
              value: "300"
            - name: INTERNAL_OTEL_TRACE_LOGGING_ENABLED
              value: "0"
            - name: INTERNAL_OTEL_TRACE_SAMPLING_RATE
              value: "20"
            - name: INTERNAL_OTEL_TRACE_INSTRUMENT_PRISMA_ENABLED
              value: "0"
            - name: INTERNAL_OTEL_TRACE_DISABLED
              value: "0"
            - name: INTERNAL_OTEL_METRIC_EXPORTER_ENABLED
              value: "0"
            - name: INTERNAL_OTEL_METRIC_EXPORTER_INTERVAL_MS
              value: "30000"
            - name: CLICKHOUSE_URL
              value: "http://default:password@trigger-clickhouse:8123?secure=false"
            - name: CLICKHOUSE_LOG_LEVEL
              value: "info"
            - name: RUN_REPLICATION_ENABLED
              value: "1"
            - name: RUN_REPLICATION_CLICKHOUSE_URL
              value: "http://default:password@trigger-clickhouse:8123"
            - name: RUN_REPLICATION_LOG_LEVEL
              value: "info"
          volumeMounts:
            - name: shared
              mountPath: /home/node/shared
      volumes:
        - name: shared
          persistentVolumeClaim:
            claimName: trigger-shared
---
# Source: trigger/charts/clickhouse/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: trigger-clickhouse-shard0
  namespace: "trigger"
  labels:
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/version: 25.6.1
    helm.sh/chart: clickhouse-9.3.7
    app.kubernetes.io/component: clickhouse
    app.kubernetes.io/part-of: clickhouse
    shard: "0"
spec:
  replicas: 1
  podManagementPolicy: "Parallel"
  selector:
    matchLabels:
      app.kubernetes.io/instance: trigger
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/component: clickhouse
      app.kubernetes.io/part-of: clickhouse
      shard: "0"
  serviceName: trigger-clickhouse-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/configd-configuration: 51c61dc766833a24598911606ad88f5aa4dbdc2263d6adb7d8ab47dbf67e4954
      labels:
        app.kubernetes.io/instance: trigger
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: clickhouse
        app.kubernetes.io/version: 25.6.1
        helm.sh/chart: clickhouse-9.3.7
        app.kubernetes.io/component: clickhouse
        app.kubernetes.io/part-of: clickhouse
        shard: "0"
    spec:
      
      automountServiceAccountToken: false
      serviceAccountName: trigger-clickhouse
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: trigger
                    app.kubernetes.io/name: clickhouse
                    app.kubernetes.io/component: clickhouse
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
      containers:
        - name: clickhouse
          image: docker.io/bitnami/clickhouse:25.6.1-debian-12-r0
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: CLICKHOUSE_HTTP_PORT
              value: "8123"
            - name: CLICKHOUSE_TCP_PORT
              value: "9000"
            - name: CLICKHOUSE_MYSQL_PORT
              value: "9004"
            - name: CLICKHOUSE_POSTGRESQL_PORT
              value: "9005"
            - name: CLICKHOUSE_INTERSERVER_HTTP_PORT
              value: "9009"
            - name: CLICKHOUSE_SHARD_ID
              value: "shard0"
            - name: CLICKHOUSE_REPLICA_ID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: CLICKHOUSE_ADMIN_USER
              value: "default"
            - name: CLICKHOUSE_ADMIN_PASSWORD_FILE
              value: /opt/bitnami/clickhouse/secrets/admin-password
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          ports:
            - name: http
              containerPort: 8123
            - name: tcp
              containerPort: 9000
            - name: tcp-postgresql
              containerPort: 9005
            - name: tcp-mysql
              containerPort: 9004
            - name: http-intersrv
              containerPort: 9009
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            tcpSocket:
              port: http
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            httpGet:
              path: /ping
              port: http
          volumeMounts:
            - name: data
              mountPath: /bitnami/clickhouse
            - name: configd-configuration
              mountPath: /bitnami/clickhouse/etc/config.d
            - name: secrets
              mountPath: /opt/bitnami/clickhouse/secrets
              readOnly: true
            - name: empty-dir
              mountPath: /opt/bitnami/clickhouse/etc
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /opt/bitnami/clickhouse/logs
              subPath: app-logs-dir
            - name: empty-dir
              mountPath: /opt/bitnami/clickhouse/tmp
              subPath: app-tmp-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
      volumes:
        - name: empty-dir
          emptyDir: {}
        - name: configd-configuration
          configMap:
            name: trigger-clickhouse-configd
        - name: secrets
          secret:
            secretName: trigger-clickhouse
            defaultMode: 256
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: data
        labels:
          app.kubernetes.io/instance: trigger
          app.kubernetes.io/name: clickhouse
          app.kubernetes.io/component: clickhouse
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "10Gi"
---
# Source: trigger/templates/validate-external-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: trigger-external-config-validation
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-10"
    helm.sh/hook-delete-policy: before-hook-creation
data:
  validation: "completed"
---
# Source: trigger/templates/tests/test-clickhouse.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "trigger-test-clickhouse"
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  restartPolicy: Never
  containers:
    - name: test-clickhouse
      image: curlimages/curl:8.14.1
      command: ['sh', '-c']
      args:
        - |
          echo "Testing ClickHouse HTTP interface..."
          curl -f --user ":" "http://trigger-clickhouse:8123/ping"
          echo "ClickHouse test completed successfully"
---
# Source: trigger/templates/tests/test-electric.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "trigger-test-electric"
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  restartPolicy: Never
  containers:
    - name: test-electric
      image: curlimages/curl:8.14.1
      command: ['sh', '-c']
      args:
        - |
          echo "Testing Electric health endpoint..."
          curl -f http://trigger-electric:3000/api/status
          echo "Electric test completed successfully"
---
# Source: trigger/templates/tests/test-webapp.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "trigger-test-webapp"
  labels:
    helm.sh/chart: trigger-4.0.0
    app.kubernetes.io/name: trigger
    app.kubernetes.io/instance: trigger
    app.kubernetes.io/version: "v4.0.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  restartPolicy: Never
  containers:
    - name: test-webapp
      image: curlimages/curl:8.14.1
      command: ['sh', '-c']
      args:
        - |
          echo "Testing webapp health endpoint..."
          curl -f http://trigger-webapp:3030/healthcheck
          echo "Webapp test completed successfully"
