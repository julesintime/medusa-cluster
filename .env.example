# LiteLLM OpenAI-Compatible API Configuration
# Copy this file to .env and update with your actual API key

# OpenAI-compatible endpoint (LiteLLM proxy)
OPENAI_API_BASE=https://litellm.xuperson.org/v1
OPENAI_BASE_URL=https://litellm.xuperson.org/v1

# Your LiteLLM master key (use this exact key)
OPENAI_API_KEY=lo2v6ewnDLY2JXapRNTqdYZGs6Up2kHmzGfGbw5STr8=

# Model to use - multiple routing options available:
# - Specific models: gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash, etc.
# - Wildcard: gemini/any-model-name (routes via gemini/* pattern)  
# - Default rotation: gpt-4 (rotates through all models)
OPENAI_MODEL=gpt-4

# Optional: Organization ID (if configured in LiteLLM)
# OPENAI_ORGANIZATION=your-org-id

# Example usage with different libraries:

# Python (openai library):
# import openai
# openai.api_base = "https://litellm.xuperson.org/v1"
# openai.api_key = "lo2v6ewnDLY2JXapRNTqdYZGs6Up2kHmzGfGbw5STr8="
# response = openai.ChatCompletion.create(
#     model="gpt-4",
#     messages=[{"role": "user", "content": "Hello!"}]
# )

# Node.js (openai library):
# const openai = new OpenAI({
#   apiKey: "lo2v6ewnDLY2JXapRNTqdYZGs6Up2kHmzGfGbw5STr8=",
#   baseURL: "https://litellm.xuperson.org/v1"
# });

# curl example:
# curl https://litellm.xuperson.org/v1/chat/completions \
#   -H "Authorization: Bearer lo2v6ewnDLY2JXapRNTqdYZGs6Up2kHmzGfGbw5STr8=" \
#   -H "Content-Type: application/json" \
#   -d '{"model": "gpt-4", "messages": [{"role": "user", "content": "Hello!"}]}'

# Available endpoints (OpenAI-compatible):
# - /v1/chat/completions - Chat completions
# - /v1/completions - Text completions  
# - /v1/models - List available models
# - /v1/embeddings - Generate embeddings (if configured)

# Available routing modes:
#
# 1. SPECIFIC MODEL ROUTING - Direct access to individual models:
#    curl -d '{"model": "gemini-2.5-pro", "messages": [...]}' ...
#    curl -d '{"model": "gemini-2.0-flash", "messages": [...]}' ...
#    curl -d '{"model": "gemma-3-27b-it", "messages": [...]}' ...
#
# 2. WILDCARD ROUTING - Use any gemini model via pattern:
#    curl -d '{"model": "gemini/gemini-2.5-pro", "messages": [...]}' ...
#    curl -d '{"model": "gemini/new-future-model", "messages": [...]}' ...
#
# 3. DEFAULT ROTATION - Load balances across ALL models:
#    curl -d '{"model": "gpt-4", "messages": [...]}' ...
#    curl -d '{"model": "any-unknown-name", "messages": [...]}' ...
#
# 4. AUTOMATIC FAILOVER - All specific models fall back to rotation pool
#
# Available models (each with 2 API keys for load balancing):
# - gemini-2.5-pro (5 RPM, Enhanced reasoning)
# - gemini-2.5-flash (10 RPM, Cost efficient)  
# - gemini-2.5-flash-lite (15 RPM, Most cost efficient)
# - gemini-2.0-flash (15 RPM, High throughput)
# - gemini-2.0-flash-lite (30 RPM, Highest rate limits)
# - gemma-3n-e2b-it (30 RPM, Efficient 2B open model)
# - gemma-3n-e4b-it (30 RPM, Efficient 4B open model)
# - gemma-3-27b-it (30 RPM, Large 27B open model)