# LiteLLM OpenAI-Compatible API Configuration
# Copy this file to .env and update with your actual API key

# OpenAI-compatible endpoint (LiteLLM proxy)
OPENAI_API_BASE=https://litellm.xuperson.org/v1
OPENAI_BASE_URL=https://litellm.xuperson.org/v1

# Your LiteLLM master key (use this exact key)
OPENAI_API_KEY=lo2v6ewnDLY2JXapRNTqdYZGs6Up2kHmzGfGbw5STr8=

# Model to use (all models are exposed as "gpt-4" in the config)
OPENAI_MODEL=gpt-4

# Optional: Organization ID (if configured in LiteLLM)
# OPENAI_ORGANIZATION=your-org-id

# Example usage with different libraries:

# Python (openai library):
# import openai
# openai.api_base = "https://litellm.xuperson.org/v1"
# openai.api_key = "lo2v6ewnDLY2JXapRNTqdYZGs6Up2kHmzGfGbw5STr8="
# response = openai.ChatCompletion.create(
#     model="gpt-4",
#     messages=[{"role": "user", "content": "Hello!"}]
# )

# Node.js (openai library):
# const openai = new OpenAI({
#   apiKey: "lo2v6ewnDLY2JXapRNTqdYZGs6Up2kHmzGfGbw5STr8=",
#   baseURL: "https://litellm.xuperson.org/v1"
# });

# curl example:
# curl https://litellm.xuperson.org/v1/chat/completions \
#   -H "Authorization: Bearer lo2v6ewnDLY2JXapRNTqdYZGs6Up2kHmzGfGbw5STr8=" \
#   -H "Content-Type: application/json" \
#   -d '{"model": "gpt-4", "messages": [{"role": "user", "content": "Hello!"}]}'

# Available endpoints (OpenAI-compatible):
# - /v1/chat/completions - Chat completions
# - /v1/completions - Text completions  
# - /v1/models - List available models
# - /v1/embeddings - Generate embeddings (if configured)

# Behind the scenes, LiteLLM rotates through these models:
# - gemini-2.5-pro (Enhanced reasoning)
# - gemini-2.5-flash (Cost efficient)
# - gemini-2.5-flash-lite (Most cost efficient)
# - gemini-2.0-flash (High throughput)
# - gemini-2.0-flash-lite (Highest rate limits)
# - gemma-3 (Open model)
# - gemma-3n (Open model variant)
# Each model uses 2 API keys for load balancing